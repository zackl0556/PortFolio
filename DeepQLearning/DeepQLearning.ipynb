{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "088c96b4",
   "metadata": {},
   "source": [
    "# Nombres de los estudiantes\n",
    "\n",
    "Podéis hacer la práctica de forma individual o en parejas. Si trabajáis en parejas basta con que la entregue uno de los dos. \n",
    "El profesor puede pediros que defendáis vuestra entrega en cualquier momento para comprobar que la habéis hecho vosotros y la entendéis. La entrega de prácticas demasiado similares o con soluciones que no sabéis explicar puede tener serias consecuencias en la calificación final de la asignatura. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f13d97",
   "metadata": {},
   "source": [
    "# Deep Q-Learning\n",
    "\n",
    "## Entorno\n",
    "\n",
    "En este notebook vamos a usar el entorno [Cartpole](https://gymnasium.farama.org/environments/classic_control/cart_pole/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692b5562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c48cd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "print(\"Espacio de observación:\", env.observation_space)\n",
    "print(\"  - Shape:\", env.observation_space.shape)\n",
    "print(\"  - Ejemplo:\", env.observation_space.sample())\n",
    "print()\n",
    "print(\"Espacio de acciones:\", env.action_space)\n",
    "print(\"  - Número de acciones:\", env.action_space.n)\n",
    "print(\"  - 0: Empujar izquierda\")\n",
    "print(\"  - 1: Empujar derecha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e143d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "plt.axis('off')\n",
    "plt.imshow(env.render())\n",
    "display.display(plt.gcf())  \n",
    "display.clear_output(wait=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e554f954",
   "metadata": {},
   "source": [
    "## Red neuronal\n",
    "\n",
    "Vamos a comprobar la versión de pytorch y si tenemos aceleración HW.\n",
    "\n",
    "Usaremos la variable global DEVICE para mover todos los modelos y datos al dispositivo más rápido antes de operar con ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ffbb74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() \\\n",
    "        else 'mps' if torch.mps.is_available() \\\n",
    "        else 'cpu'\n",
    "\n",
    "print(\"Dispositivo disponible:\",  DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4650fff",
   "metadata": {},
   "source": [
    "A continuación creamos la red neuronal que usaremos para representar la política."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd7bddc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Red neuronal feedforward para aproximar Q-values.\n",
    "    \n",
    "    Arquitectura:\n",
    "    - Capa de entrada: recibe el estado del entorno\n",
    "    - 2 capas ocultas Linear con activación ReLU\n",
    "    - Capa de salida Linear: devuelve Q-value para cada acción posible\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    state_size : int\n",
    "        Dimensión del espacio de estados (número de features de observación)\n",
    "    action_size : int\n",
    "        Número de acciones posibles\n",
    "    hidden_size : int, opcional\n",
    "        Número de neuronas en las capas ocultas (default: 128)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_size: int, action_size: int, hidden_size: int = 128):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "\n",
    "        # TODO TODO\n",
    "        # Capa de tipo secuencial que contiene las capas de la red\n",
    "        # Lineal + ReLU + Lineal + ReLU + Lineal\n",
    "        self.seq = nn.Sequential(\n",
    "               nn.Linear(state_size,hidden_size),\n",
    "               nn.ReLU(),\n",
    "               nn.Linear(hidden_size,hidden_size),\n",
    "               nn.ReLU(),\n",
    "               nn.Linear(hidden_size,action_size)\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass de la red.\n",
    "        \n",
    "        Parámetros\n",
    "        ----------\n",
    "        state : torch.Tensor\n",
    "            Estado(s) del entorno. Shape: (batch_size, state_size) o (state_size,)\n",
    "            \n",
    "        Retorna\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Q-values para cada acción. Shape: (batch_size, action_size) o (action_size,)\n",
    "        \"\"\"\n",
    "        # TODO TODO\n",
    "        # Aplica la red neuronal al estado de entrada y devuelve el resultado\n",
    "        return self.seq(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65534a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO TOCAR\n",
    "net = DQNNetwork(10, 5, 20)\n",
    "\n",
    "assert isinstance(net.seq[0], torch.nn.Linear), \"Capa 0 debe ser Linear\"\n",
    "assert isinstance(net.seq[1], torch.nn.ReLU), \"Capa 1 debe ser ReLU\"\n",
    "assert isinstance(net.seq[2], torch.nn.Linear), \"Capa 2 debe ser Linear\"\n",
    "assert isinstance(net.seq[3], torch.nn.ReLU), \"Capa 3 debe ser ReLU\"\n",
    "assert isinstance(net.seq[4], torch.nn.Linear), \"Capa 4 debe ser Linear\"\n",
    "\n",
    "assert net.seq[0].in_features == 10, \"Dimensiones incorrectas\"\n",
    "assert net.seq[0].out_features == 20, \"Dimensiones incorrectas\"\n",
    "assert net.seq[2].in_features == 20, \"Dimensiones incorrectas\"\n",
    "assert net.seq[2].out_features == 20, \"Dimensiones incorrectas\"\n",
    "assert net.seq[4].in_features == 20, \"Dimensiones incorrectas\"\n",
    "assert net.seq[4].out_features == 5, \"Dimensiones incorrectas\"\n",
    "\n",
    "state = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=torch.float32)\n",
    "q_values = net(state)\n",
    "assert torch.equal(q_values, net.seq[4](net.seq[3](net.seq[2](net.seq[1](net.seq[0](state)))))), \"error en el método forward\"\n",
    "del state, net, q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc86b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una red de ejemplo\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "example_net = DQNNetwork(state_size, action_size, hidden_size=128)\n",
    "print(example_net)\n",
    "print()\n",
    "\n",
    "# Contar parámetros\n",
    "total_params = sum(p.numel() for p in example_net.parameters())\n",
    "print(f\"Total de parámetros entrenables: {total_params:,}\")\n",
    "print()\n",
    "\n",
    "# Probar forward pass\n",
    "example_state = np.array(env.reset()[0])\n",
    "example_state = torch.FloatTensor(example_state)\n",
    "q_values = example_net(example_state)\n",
    "print(f\"Estado de entrada shape: {example_state.shape}\")\n",
    "print(f\"Q-values de salida shape: {q_values.shape}\")\n",
    "print(f\"Q-values: {q_values.detach().numpy()}\")\n",
    "print(f\"Acción greedy: {q_values.argmax().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67545539",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "\n",
    "Necesitamos una memoria donde almacenar las experiencias con las que aprender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51766285",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Buffer circular para almacenar y samplear experiencias.\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    capacity : int\n",
    "        Tamaño máximo del buffer. Cuando se llena, las experiencias\n",
    "        más antiguas se eliminan automáticamente.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int):\n",
    "        # TODO TODO\n",
    "        # Crea un buffer circular usando una cola (deque)\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state: np.ndarray, action: int, reward: float, \n",
    "             next_state: np.ndarray, done: bool) -> None:\n",
    "        \"\"\"\n",
    "        Añade una transición al buffer.\n",
    "        \n",
    "        Parámetros\n",
    "        ----------\n",
    "        state : np.ndarray\n",
    "            Estado actual\n",
    "        action : int\n",
    "            Acción tomada\n",
    "        reward : float\n",
    "            Recompensa recibida\n",
    "        next_state : np.ndarray\n",
    "            Estado siguiente\n",
    "        done : bool\n",
    "            True si el episodio terminó, False en caso contrario\n",
    "        \"\"\"\n",
    "        # TODO TODO\n",
    "        # Añade al buffer una tupla (estado, acción, recompensa, siguiente estado, terminado)\n",
    "        self.buffer.append((state,action,reward,next_state,done))\n",
    "\n",
    "        \n",
    "    \n",
    "    def sample(self, batch_size: int) -> Tuple[np.ndarray,np.ndarray,np.ndarray,np.ndarray,np.ndarray,np.ndarray]:\n",
    "        \"\"\"\n",
    "        Samplea un mini-batch aleatorio de experiencias.\n",
    "        \n",
    "        Parámetros\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            Número de experiencias a samplear\n",
    "            \n",
    "        Retorna\n",
    "        -------\n",
    "        Tuple de arrays numpy:\n",
    "            - states: (batch_size, state_size)\n",
    "            - actions: (batch_size,)\n",
    "            - rewards: (batch_size,)\n",
    "            - next_states: (batch_size, state_size)\n",
    "            - dones: (batch_size,)\n",
    "        \"\"\"\n",
    "        # Samplear índices aleatorios\n",
    "        # TODO TODO\n",
    "        # Usa random.sample para elegir un batch de tuplas\n",
    "        batch = random.sample(self.buffer,batch_size)\n",
    "        \n",
    "        # Desempaquetar el batch en arrays separados\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states),\n",
    "            np.array(dones, dtype=np.float32)\n",
    "        )\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Retorna el número de experiencias actualmente en el buffer.\"\"\"\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45f93f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO TOCAR\n",
    "buffer = ReplayBuffer(1)\n",
    "assert len(buffer) == 0, 'el buffer no está vacío inicialmente'\n",
    "t1 = [1, 2], 3, 4, [5, 6], False\n",
    "buffer.push(*t1)\n",
    "assert len(buffer) == 1, 'el método push no almacena las experiencias'\n",
    "t2 = buffer.sample(1)\n",
    "for i in range(len(t1)):\n",
    "    assert (t2[i] == np.array([t1[i]])).all(), 'el método sample no devuelve los datos correctos'\n",
    "t3 = [10, 20], 30, 40, [50, 60], True\n",
    "buffer.push(*t3)\n",
    "assert len(buffer) == 1, 'el buffer no es circular o no tiene el tamaño correcto'\n",
    "\n",
    "buffer = ReplayBuffer(2)\n",
    "buffer.push(*t1)\n",
    "buffer.push(*t3)\n",
    "assert len(buffer) == 2, 'el método push no almacena las experiencias'\n",
    "t2 = buffer.sample(1)\n",
    "for i in range(len(t1)):\n",
    "    assert (t2[i] == np.array([t1[i]])).all() or (t2[i] == np.array([t3[i]])).all() , 'el método sample no devuelve los datos correctos'\n",
    "assert len(buffer.sample(2)[0]) == 2, 'el método sample no devuelve el número correcto de experiencias'\n",
    "del buffer, t1, t2, t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d1eec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear buffer de ejemplo\n",
    "buffer = ReplayBuffer(capacity=1000)\n",
    "\n",
    "# Llenar con algunas experiencias\n",
    "state, _ = env.reset()\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    if done:\n",
    "        state, _ = env.reset()\n",
    "    else:\n",
    "        state = next_state\n",
    "\n",
    "print(f\"Experiencias en buffer: {len(buffer)}\")\n",
    "print()\n",
    "\n",
    "# Samplear un mini-batch\n",
    "states, actions, rewards, next_states, dones = buffer.sample(batch_size=8)\n",
    "print(\"Mini-batch sampled:\")\n",
    "print(f\"  States shape: {states.shape}\")\n",
    "print(f\"  Actions shape: {actions.shape}\")\n",
    "print(f\"  Rewards: {rewards}\")\n",
    "print(f\"  Dones: {dones}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc814a0b",
   "metadata": {},
   "source": [
    "## Selección de la acción\n",
    "\n",
    "Vamos a implementar una política epsilon greedy con la policy_net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9234390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(policy_net: DQNNetwork, state: np.ndarray, epsilon: float, action_size: int) -> int:\n",
    "    \"\"\"\n",
    "    Selecciona una acción usando epsilon-greedy policy.\n",
    "    \n",
    "    Con probabilidad epsilon elige una acción aleatoria (exploración), y con probabilidad \n",
    "    1-epsilon elige la  mejor acción según la red (explotación).\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    policy_net : DQNNetwork\n",
    "        Red neuronal que representa la política del agente\n",
    "    state : np.ndarray\n",
    "        Estado actual del entorno como array\n",
    "    epsilon : float\n",
    "        Probabilidad de elegir una acción aleatoria\n",
    "    action_size : int\n",
    "        Número de acciones disponibles en el entorno\n",
    "        \n",
    "    Retorna\n",
    "    -------\n",
    "    int\n",
    "        Acción seleccionada\n",
    "    \"\"\"\n",
    "    # Exploración: acción aleatoria\n",
    "    if np.random.random() < epsilon:\n",
    "        # TODO TODO\n",
    "        # Devolver acción aleatoria con np.random.randint\n",
    "        return np.random.randint(0,action_size)\n",
    "    \n",
    "    # Explotación: mejor acción según Q-values\n",
    "    with torch.no_grad():\n",
    "        # Transformar el estado a un tensor\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        # TODO TODO\n",
    "        # aplicar la red al estado para obtener los valores q\n",
    "        q_values = policy_net(state)\n",
    "        \n",
    "        # devolver el índice del mayor valor Q (como int)\n",
    "        return q_values.argmax().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1e0c4356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO TOCAR\n",
    "actions = [epsilon_greedy_policy(None, None, 1, 10) for _ in range(1000)]\n",
    "assert len(set(actions)) > 1, 'No se devuelven acciones aleatorias con probabilidad epsilon'\n",
    "assert all(i in actions for i in range(10)), 'No se devuelven todas las acciones posibles del entorno'\n",
    "policy_net = DQNNetwork(4, 2, 10).to(DEVICE)\n",
    "state = np.array([1, 2, 3, 4])\n",
    "actions = [epsilon_greedy_policy(policy_net, state, 0, 10) for _ in range(1000)]\n",
    "assert len(set(actions)) == 1, \"No se devuelve la mejor acción con probabilidad 1 - epsilon\"\n",
    "del actions, policy_net, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4aa8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elegir una acción\n",
    "policy = DQNNetwork(4, 2, 10).to(DEVICE)\n",
    "state = np.array([0.1, 0.2, 0.3, 0.4])\n",
    "action = epsilon_greedy_policy(policy, state, 0.5, 10)\n",
    "print(\"Acción:\", action)\n",
    "\n",
    "# Ahora varias acciones para comprobar que UNA DE ELLAS APARECE MÁS\n",
    "acciones = [epsilon_greedy_policy(policy, state, 0.5, 10) for _ in range(20)]\n",
    "print('Acciones:', acciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac63d29",
   "metadata": {},
   "source": [
    "## Paso de entrenamiento\n",
    "\n",
    "Ahora llegamos a la parte interesante. Un paso de entrenamiento que consiste en:\n",
    "- Elegir un mini bath de experiencias\n",
    "- Calcular los valores Q de los estados del minibatch usando la policy_net\n",
    "- Calcular los nuevos valores Q usando la ecuación de Bellman y la target_net\n",
    "- Calcular el error entre ambos valores\n",
    "- Entrenar la red para disminuir el error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7113b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(policy_net: DQNNetwork, \n",
    "               target_net: DQNNetwork, \n",
    "               gamma: float, \n",
    "               optimizer: torch.optim.Optimizer, \n",
    "               memory: ReplayBuffer, \n",
    "               batch_size: int) -> float:\n",
    "    \"\"\"\n",
    "    Ejecuta un paso de entrenamiento con un mini-batch del replay buffer.\n",
    "    \n",
    "    Implementa la actualización de Q-learning:\n",
    "    Q(s,a) ← Q(s,a) + α[r + γ max_a' Q_target(s',a') - Q(s,a)]\n",
    "    \n",
    "    Parámetros\n",
    "        ----------\n",
    "        policy_net : DQNNetwork\n",
    "            Red neuronal que representa la política del agente (entrenada)\n",
    "        target_net : DQNNetwork\n",
    "            Red neuronal objetivo (fija temporalmente)\n",
    "        gamma : float\n",
    "            Factor de descuento para recompensas futuras\n",
    "        optimizer : torch.optim.Optimizer\n",
    "            Optimizador para actualizar los pesos de la red\n",
    "        memory : ReplayBuffer\n",
    "            Buffer de experiencias para samplear mini-batches\n",
    "        batch_size : int\n",
    "            Tamaño del mini-batch de entrenamiento\n",
    "        \n",
    "    Retorna\n",
    "    -------\n",
    "    float\n",
    "        Pérdida (loss) del paso de entrenamiento, o 0.0 si no hay\n",
    "        suficientes experiencias en el buffer\n",
    "    \"\"\"\n",
    "    # No entrenar si no hay suficientes experiencias\n",
    "    if len(memory) < batch_size:\n",
    "        return 0.0\n",
    "    \n",
    "    # Samplear mini-batch del replay buffer\n",
    "    # TODO TODO\n",
    "    states, actions, rewards, next_states, dones = memory.sample(batch_size)\n",
    "    assert len(states) == batch_size\n",
    "    \n",
    "    # Convertir a tensores de PyTorch\n",
    "    states = torch.FloatTensor(states).to(DEVICE)\n",
    "    actions = torch.LongTensor(actions).to(DEVICE)\n",
    "    rewards = torch.FloatTensor(rewards).to(DEVICE)\n",
    "    next_states = torch.FloatTensor(next_states).to(DEVICE)\n",
    "    dones = torch.FloatTensor(dones).to(DEVICE)\n",
    "    \n",
    "    # Q-values actuales: Q(s,a) para las acciones tomadas\n",
    "    # TODO TODO\n",
    "    # aplicar la policy_net a los estados del mini batch y obtener sus valores Q\n",
    "    current_q_values = policy_net(states)\n",
    "    # seleccionar los valores Q de la acción tomada en cada estado\n",
    "    current_q_values = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "    # Q-values objetivo: r + γ max_a' Q_target(s',a')\n",
    "    with torch.no_grad():\n",
    "        next_q_values = target_net(next_states).max(1)[0]\n",
    "        # Si dones = 1 no hay estado siguiente y target_q_values = rewards\n",
    "        target_q_values = rewards + (1 - dones) * gamma * next_q_values\n",
    "    \n",
    "    # Calcular pérdida y actualizar pesos\n",
    "    # TODO TODO\n",
    "    # Usa F.mse_loss para calcular el error entre current_q_values y target_q_values\n",
    "    loss = F.mse_loss(current_q_values,target_q_values)\n",
    "    \n",
    "    # minimizar el error\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e19eef",
   "metadata": {},
   "source": [
    "Vamos a probar con datos ficticios. El error debería disminuir tras cada paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4ba81cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Es importante que la red y los datos estén en el mismo dispositivo\n",
    "policy_net = DQNNetwork(4, 2, 10).to(DEVICE)    \n",
    "target_net = DQNNetwork(4, 2, 10).to(DEVICE)\n",
    "optimizer = torch.optim.SGD(policy_net.parameters(), lr=0.1)\n",
    "gamma = 0.9\n",
    "memory = ReplayBuffer(100)\n",
    "memory.push([0.1, 0.2, 0.3, 0.4], 0, 1, [0.5, 0.6, 0.7, 0.8], False)\n",
    "batch_size = 1\n",
    "\n",
    "loss1 = train_step(policy_net, target_net, gamma, optimizer, memory, batch_size)\n",
    "print('Error tras 1 paso:', loss1)\n",
    "\n",
    "loss2 = train_step(policy_net, target_net, gamma, optimizer, memory, batch_size)\n",
    "print('Error tras 2 pasos:', loss2)\n",
    "\n",
    "loss3 = train_step(policy_net, target_net, gamma, optimizer, memory, batch_size)\n",
    "print('Error tras 3 pasos:', loss3)\n",
    "\n",
    "assert loss1 > loss2 > loss3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc956c2",
   "metadata": {},
   "source": [
    "## Copia de pesos\n",
    "\n",
    "Durante el entrenamiento debemos copiar los pesos de la policy_net en la target_net. Vamos a hacer una función para hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8e3aa4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_weights(policy_net: DQNNetwork, target_net: DQNNetwork) -> None:\n",
    "    \"\"\"Copia los pesos de la policy network a la target network.\"\"\"\n",
    "    # TODO TODO\n",
    "    # Usa el método state_dict() para obtener los pesos de una red\n",
    "    # Usa el método load_state_dict() para establecer los pesos de una red\n",
    "    \n",
    "    target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b7393af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO TOCAR\n",
    "policy_net = DQNNetwork(4, 2, 10).to(DEVICE)\n",
    "target_net = DQNNetwork(4, 2, 10).to(DEVICE)\n",
    "sd = policy_net.state_dict()\n",
    "copy_weights(policy_net, target_net)\n",
    "psd = policy_net.state_dict()\n",
    "tsd = target_net.state_dict()\n",
    "assert psd.keys() == sd.keys() and all(torch.equal(psd[k], sd[k]) for k in psd), 'policy_net ha cambiado y no debería'\n",
    "assert tsd.keys() == sd.keys() and all(torch.equal(tsd[k], sd[k]) for k in tsd), 'target_net no es igual que policy_net'\n",
    "del policy_net, target_net, sd, psd, tsd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f1c88",
   "metadata": {},
   "source": [
    "## Entrenamiento\n",
    "\n",
    "Finalmente vamos a entrenar nuestro agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4e2a7e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env: gym.Env, n_episodes: int, lr: float, gamma: float, \n",
    "          epsilon: float, epsilon_decay: float, epsilon_min: float, \n",
    "          memory_size: int, batch_size: int, \n",
    "          target_update_steps: int, \n",
    "          print_every: int,\n",
    "          final_model_path: str):\n",
    "    \"\"\"\n",
    "    Entrena un agente DQN en el entorno especificado.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        Entorno de OpenAI Gymnasium.\n",
    "    n_episodes : int\n",
    "        Número de episodios de entrenamiento.\n",
    "    lr : float\n",
    "        Tasa de aprendizaje para el optimizador.\n",
    "    gamma : float\n",
    "        Factor de descuento para recompensas futuras.\n",
    "    epsilon : float\n",
    "        Valor inicial de epsilon para la política epsilon-greedy.\n",
    "    epsilon_decay : float\n",
    "        Factor de decaimiento de epsilon por episodio.\n",
    "    epsilon_min : float\n",
    "        Valor mínimo de epsilon.\n",
    "    memory_size : int\n",
    "        Capacidad máxima del replay buffer.\n",
    "    batch_size : int\n",
    "        Tamaño del mini-batch para entrenamiento.\n",
    "    target_update_steps : int\n",
    "        Frecuencia (en pasos) para actualizar la red objetivo.\n",
    "    print_every : int\n",
    "        Frecuencia (en episodios) para imprimir métricas de entrenamiento.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    policy_net : DQNNetwork\n",
    "        Red neuronal entrenada que representa la política aprendida.\n",
    "    \"\"\"\n",
    "    \n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    # Redes\n",
    "    policy_net = DQNNetwork(obs_size, action_size).to(DEVICE)\n",
    "    target_net = DQNNetwork(obs_size, action_size).to(DEVICE)\n",
    "    copy_weights(policy_net, target_net)\n",
    "    target_net.eval()\n",
    "    \n",
    "    # Optimizador\n",
    "    optimizer = torch.optim.Adam(policy_net.parameters(), lr=lr)\n",
    "\n",
    "    # Replay buffer\n",
    "    memory = ReplayBuffer(memory_size)\n",
    "\n",
    "    # Estadísticas\n",
    "    episode_rewards = []\n",
    "    episode_losses = []\n",
    "    avg_rewards = []\n",
    "    steps = 0\n",
    "\n",
    "    print(\"Iniciando entrenamiento...\")\n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_loss = []\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Seleccionar y ejecutar acción\n",
    "            action = epsilon_greedy_policy(policy_net, state, epsilon, action_size)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # Almacenar experiencia\n",
    "            memory.push(state, action, reward, next_state, done)\n",
    "\n",
    "            # Entrenamiento\n",
    "            loss = train_step(policy_net, target_net, gamma, optimizer, memory, batch_size)\n",
    "            steps += 1\n",
    "\n",
    "            # Actualización de la target_net\n",
    "            if steps % target_update_steps == 0:\n",
    "                copy_weights(policy_net, target_net)\n",
    "\n",
    "            if loss > 0:\n",
    "                episode_loss.append(loss)\n",
    "            episode_reward += reward\n",
    "\n",
    "            state = next_state\n",
    "        \n",
    "        # Disminuir epsilon\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "        # Guardar métricas\n",
    "        episode_rewards.append(episode_reward)\n",
    "        avg_loss = np.mean(episode_loss) if episode_loss else 0\n",
    "        episode_losses.append(avg_loss)\n",
    "        \n",
    "        # Promedio móvil\n",
    "        avg_reward = np.mean(episode_rewards[-100:])\n",
    "        avg_rewards.append(avg_reward)\n",
    "\n",
    "        # Imprimir progreso\n",
    "        if episode % print_every == 0:\n",
    "            print(f\"Episodio {episode:4d} | Recompensa: {episode_reward:6.1f} | \"\n",
    "                f\"Avg(100): {avg_reward:6.2f} | Epsilon: {epsilon:.3f} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"\\n¡Entrenamiento completado!\")\n",
    "    \n",
    "    # Salvar modelo\n",
    "    torch.save(policy_net, final_model_path)\n",
    "    print(f\"\\nModelo salvado en {final_model_path}\")\n",
    "\n",
    "    return episode_rewards, avg_rewards, episode_losses, avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bf113d",
   "metadata": {},
   "source": [
    "El entrenamiento es muy poco estable. Lo importante es comprobar que la recompensa media de los últimos 100 episodios va aumentando.\n",
    "\n",
    "Ejecutar la siguiente celda puede llevar un rato... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8bd4987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"./models/\", exist_ok=True)\n",
    "\n",
    "# Configuración de entrenamiento\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "n_episodes = 500\n",
    "lr = 0.001\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "memory_size = 10_000\n",
    "batch_size = 64\n",
    "target_update_steps = 20\n",
    "print_every = 50\n",
    "final_model_path = 'models/dqn_cartpole'\n",
    "\n",
    "episode_rewards, avg_rewards, episode_losses, avg_loss = train(\n",
    "    env, n_episodes, lr, gamma, epsilon, epsilon_decay, epsilon_min, \n",
    "    memory_size, batch_size, target_update_steps, print_every, final_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a378ce",
   "metadata": {},
   "source": [
    "## 8. Visualizar Resultados del Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aebdb58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Gráfica de recompensas\n",
    "episodes = range(1, len(episode_rewards) + 1)\n",
    "ax1.plot(episodes, episode_rewards, alpha=0.4, label='Recompensa por episodio')\n",
    "ax1.plot(episodes, avg_rewards, linewidth=2, label='Promedio móvil (100 episodios)')\n",
    "ax1.axhline(y=195, color='r', linestyle='--', linewidth=2, label='Objetivo (195)')\n",
    "ax1.set_xlabel('Episodio', fontsize=12)\n",
    "ax1.set_ylabel('Recompensa', fontsize=12)\n",
    "ax1.set_title('Recompensa durante el entrenamiento DQN', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Gráfica de pérdida\n",
    "ax2.plot(episodes, episode_losses, alpha=0.7, color='orange')\n",
    "ax2.set_xlabel('Episodio', fontsize=12)\n",
    "ax2.set_ylabel('Pérdida (MSE)', fontsize=12)\n",
    "ax2.set_title('Pérdida durante el entrenamiento', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estadísticas finales\n",
    "print(f\"\\nEstadísticas finales:\")\n",
    "print(f\"  Total de episodios: {len(episode_rewards)}\")\n",
    "print(f\"  Recompensa máxima: {max(episode_rewards):.0f}\")\n",
    "print(f\"  Recompensa promedio (últimos 100): {np.mean(episode_rewards[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb7b2e",
   "metadata": {},
   "source": [
    "## Vamos a verlo jugar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ecc2ba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(env: gym.Env, policy_net: DQNNetwork) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Simula un episodio usando una política e-greedy.\n",
    "\n",
    "    Parámetros:\n",
    "        env: entorno de gym.\n",
    "        policy_net: red que define la política del agente.\n",
    "\n",
    "    Devuelve:\n",
    "        Tupla (recompensa_total, pasos_totales) .\n",
    "    \"\"\"\n",
    "    state, info = env.reset()\n",
    "    episode_steps = episode_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        # TODO TODO\n",
    "        # Selecciona la acción adecuada usando la función epsilon_greedy_policy\n",
    "        # IMPORTANTE: epsilon tiene que ser 0 para que no haga acciones aleatorias\n",
    "        action = epsilon_greedy_policy(policy_net,state,epsilon=0, action_size=env.action_space.n)\n",
    "\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_steps += 1\n",
    "        episode_reward += reward\n",
    "        done = terminated or truncated\n",
    "        state = next_state\n",
    "    return episode_reward, episode_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6d25a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo entrenado\n",
    "policy_net = torch.load(final_model_path, weights_only=False)\n",
    "policy_net.eval()\n",
    "\n",
    "# Entorno con wrapper de video\n",
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(env, episode_trigger=lambda episode_id: True, video_folder='videos/dqn', name_prefix='cartpole')\n",
    "\n",
    "# Ejecutar una partida \n",
    "episode_reward, episode_steps = play_episode(env, policy_net)\n",
    "print(f\"Recompensa del episodio: {episode_reward}\")\n",
    "print(f\"Pasos del episodio: {episode_steps}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbb87c2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
